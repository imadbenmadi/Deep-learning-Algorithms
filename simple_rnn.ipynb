{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55d7919c",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) with PyTorch\n",
    "\n",
    "This notebook provides a comprehensive guide to understanding and implementing Recurrent Neural Networks (RNNs) using PyTorch. We'll start with the basic concepts and gradually build up to more complex implementations.\n",
    "\n",
    "## What we'll cover:\n",
    "\n",
    "1. Understanding RNN architecture and concepts\n",
    "2. Building a simple RNN from scratch\n",
    "3. Using PyTorch's built-in RNN modules\n",
    "4. Implementing more advanced RNN variants like LSTM and GRU\n",
    "5. Training RNN models and making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb70c3",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea38a8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c08ec6",
   "metadata": {},
   "source": [
    "## 2. Understanding RNN Architecture\n",
    "\n",
    "### What is a Recurrent Neural Network?\n",
    "\n",
    "A Recurrent Neural Network (RNN) is a type of neural network designed to work with sequential data. Unlike feedforward neural networks, RNNs have connections that feed back into the network, creating a form of memory that allows information to persist.\n",
    "\n",
    "### Key Components of RNNs:\n",
    "\n",
    "1. **Input (x)**: The current input in the sequence\n",
    "2. **Hidden state (h)**: The \"memory\" that captures information from previous inputs\n",
    "3. **Output (y)**: The prediction for the current step\n",
    "\n",
    "### How Information Flows in an RNN:\n",
    "\n",
    "For each time step t:\n",
    "- The RNN takes the current input x_t and the previous hidden state h_{t-1}\n",
    "- Combines them to update the current hidden state h_t\n",
    "- Produces an output y_t based on h_t\n",
    "\n",
    "### Mathematical Representation:\n",
    "\n",
    "\\begin{align}\n",
    "h_t &= \\tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h) \\\\\n",
    "y_t &= W_{hy} h_t + b_y\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "- W_{xh}, W_{hh}, and W_{hy} are weight matrices\n",
    "- b_h and b_y are bias vectors\n",
    "- tanh is an activation function\n",
    "\n",
    "### Why Use RNNs?\n",
    "\n",
    "RNNs are particularly useful for:\n",
    "- Time series prediction\n",
    "- Natural language processing\n",
    "- Speech recognition\n",
    "- And other tasks involving sequential or temporal data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903a848c",
   "metadata": {},
   "source": [
    "Let's visualize the RNN architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131da8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple visualization of an unfolded RNN\n",
    "def plot_rnn_structure():\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Hidden states\n",
    "    for t in range(4):\n",
    "        # Draw the hidden state circle\n",
    "        circle = plt.Circle((t, 1), 0.2, fill=True, color='skyblue', alpha=0.8)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(t, 1, f'h{t}', ha='center', va='center')\n",
    "        \n",
    "        # Draw input\n",
    "        if t > 0:\n",
    "            ax.arrow(t-1, 1, 0.6, 0, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "        \n",
    "        # Draw the input box\n",
    "        if t > 0:\n",
    "            rect = plt.Rectangle((t-0.3, 0.3), 0.6, 0.4, fill=True, color='lightgreen', alpha=0.8)\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(t, 0.5, f'x{t}', ha='center', va='center')\n",
    "            \n",
    "            # Connect input to hidden state\n",
    "            ax.arrow(t, 0.7, 0, 0.1, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "            \n",
    "        # Draw the output box\n",
    "        if t > 0:\n",
    "            rect = plt.Rectangle((t-0.3, 1.6), 0.6, 0.4, fill=True, color='salmon', alpha=0.8)\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(t, 1.8, f'y{t}', ha='center', va='center')\n",
    "            \n",
    "            # Connect hidden state to output\n",
    "            ax.arrow(t, 1.2, 0, 0.4, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlim(-0.5, 3.5)\n",
    "    ax.set_ylim(0, 2.3)\n",
    "    ax.set_title('Recurrent Neural Network Architecture (Unfolded)')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.text(1.5, 0.1, 'Inputs', ha='center')\n",
    "    plt.text(1.5, 1, 'Hidden States', ha='center')\n",
    "    plt.text(1.5, 2.1, 'Outputs', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rnn_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fc0bad",
   "metadata": {},
   "source": [
    "## 3. Preparing Data\n",
    "\n",
    "For this tutorial, we'll create a simple synthetic dataset to demonstrate RNNs. We'll generate a sine wave and train our RNN to predict the next values in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1631a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sine wave dataset\n",
    "def generate_sine_wave(sample_size=1000, sequence_length=50):\n",
    "    # Generate a sine wave\n",
    "    time_steps = np.linspace(0, 100, sample_size)\n",
    "    data = np.sin(time_steps * 0.1)  # Sine wave with some frequency\n",
    "    \n",
    "    # Add some noise to make it more interesting\n",
    "    data += 0.1 * np.random.randn(sample_size)\n",
    "    \n",
    "    # Create sequences for training\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i+sequence_length])\n",
    "        y.append(data[i+sequence_length])\n",
    "    \n",
    "    # Convert to tensor and reshape for RNN input (batch_size, seq_len, features)\n",
    "    X = torch.FloatTensor(np.array(X)).view(-1, sequence_length, 1)\n",
    "    y = torch.FloatTensor(np.array(y)).view(-1, 1)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, data\n",
    "\n",
    "# Generate the data\n",
    "seq_length = 20\n",
    "X_train, y_train, X_test, y_test, sine_data = generate_sine_wave(sequence_length=seq_length)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "# Plot a sample of our sine wave data\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(sine_data[:100])\n",
    "plt.title(\"Sample of Sine Wave Data\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize a single training example\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(X_train[0].view(-1).detach().numpy(), label='Input Sequence')\n",
    "plt.scatter(seq_length, y_train[0].item(), color='red', label='Target Value')\n",
    "plt.title(\"Example of Input Sequence and Target\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae7066",
   "metadata": {},
   "source": [
    "## 4. Building a Simple RNN from Scratch\n",
    "\n",
    "To understand how RNNs work internally, let's implement a simple RNN cell from scratch using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d475abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNNCell, self).__init__()\n",
    "        \n",
    "        # Weights for the input layer\n",
    "        self.W_xh = nn.Parameter(torch.randn(input_size, hidden_size))\n",
    "        \n",
    "        # Weights for the hidden layer\n",
    "        self.W_hh = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        \n",
    "        # Bias for hidden layer\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_size))\n",
    "        \n",
    "        # Weights and bias for output layer\n",
    "        self.W_hy = nn.Parameter(torch.randn(hidden_size, output_size))\n",
    "        self.b_y = nn.Parameter(torch.zeros(output_size))\n",
    "        \n",
    "        # Activation function\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        # Calculate hidden state\n",
    "        h = self.tanh(x @ self.W_xh + h_prev @ self.W_hh + self.b_h)\n",
    "        \n",
    "        # Calculate output\n",
    "        y = h @ self.W_hy + self.b_y\n",
    "        \n",
    "        return y, h\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = SimpleRNNCell(input_size, hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x_sequence):\n",
    "        # x_sequence shape: (batch_size, sequence_length, input_size)\n",
    "        batch_size = x_sequence.size(0)\n",
    "        sequence_length = x_sequence.size(1)\n",
    "        \n",
    "        # Initialize hidden state with zeros\n",
    "        h = torch.zeros(batch_size, self.hidden_size).to(x_sequence.device)\n",
    "        \n",
    "        # Process the sequence\n",
    "        outputs = []\n",
    "        for t in range(sequence_length):\n",
    "            x_t = x_sequence[:, t, :]\n",
    "            output, h = self.cell(x_t, h)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        # Stack outputs along sequence dimension\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        \n",
    "        # Return only the last output\n",
    "        return outputs[:, -1, :]\n",
    "\n",
    "# Initialize our custom RNN\n",
    "input_size = 1  # Dimension of input features\n",
    "hidden_size = 32  # Number of hidden units\n",
    "output_size = 1  # Dimension of output\n",
    "\n",
    "# Create model instance\n",
    "custom_rnn = SimpleRNN(input_size, hidden_size, output_size).to(device)\n",
    "print(custom_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf409e2",
   "metadata": {},
   "source": [
    "Now, let's define a function to train our custom RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20368b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, epochs=100, learning_rate=0.01):\n",
    "    # Move data to device\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train our custom RNN model\n",
    "custom_rnn_losses = train_model(custom_rnn, X_train, y_train, epochs=100)\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(custom_rnn_losses)\n",
    "plt.title('Training Loss (Custom RNN)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7f34ce",
   "metadata": {},
   "source": [
    "## 5. Using PyTorch's RNN Modules\n",
    "\n",
    "Now that we understand the basic mechanics of an RNN, let's leverage PyTorch's built-in RNN modules which are optimized for performance and ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b490fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0):\n",
    "        super(PyTorchRNN, self).__init__()\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,  # input shape (batch_size, seq_len, features)\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Store parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward pass through RNN\n",
    "        # out shape = (batch_size, seq_length, hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # We only need the last time step's output\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Initialize the PyTorch RNN model\n",
    "rnn_model = PyTorchRNN(\n",
    "    input_size=1,\n",
    "    hidden_size=32,\n",
    "    output_size=1,\n",
    "    num_layers=1\n",
    ").to(device)\n",
    "\n",
    "print(rnn_model)\n",
    "\n",
    "# Train the PyTorch RNN model\n",
    "rnn_losses = train_model(rnn_model, X_train, y_train, epochs=100)\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rnn_losses)\n",
    "plt.title('Training Loss (PyTorch RNN)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b744c9",
   "metadata": {},
   "source": [
    "## 6. Training the RNN Model\n",
    "\n",
    "We've already trained our models with basic parameters. Now, let's create a more comprehensive training function with evaluation and implement early stopping to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69664bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_validation(model, X_train, y_train, X_val=None, y_val=None, \n",
    "                         epochs=100, learning_rate=0.01, patience=10):\n",
    "    # If validation data not provided, use 20% of training data\n",
    "    if X_val is None or y_val is None:\n",
    "        val_size = int(len(X_train) * 0.2)\n",
    "        indices = torch.randperm(len(X_train))\n",
    "        train_indices = indices[val_size:]\n",
    "        val_indices = indices[:val_size]\n",
    "        \n",
    "        X_val, y_val = X_train[val_indices], y_train[val_indices]\n",
    "        X_train, y_train = X_train[train_indices], y_train[train_indices]\n",
    "    \n",
    "    # Move data to device\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_val = X_val.to(device)\n",
    "    y_val = y_val.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # For early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve_epochs = 0\n",
    "    best_model = None\n",
    "    \n",
    "    # For tracking metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        train_outputs = model(X_train)\n",
    "        train_loss = criterion(train_outputs, y_train)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "        \n",
    "        # Track losses\n",
    "        train_losses.append(train_loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], '\n",
    "                  f'Train Loss: {train_loss.item():.4f}, '\n",
    "                  f'Val Loss: {val_loss.item():.4f}')\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improve_epochs = 0\n",
    "            best_model = model.state_dict().copy()\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            \n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            model.load_state_dict(best_model)\n",
    "            break\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Initialize a fresh model\n",
    "rnn_model_with_val = PyTorchRNN(\n",
    "    input_size=1,\n",
    "    hidden_size=32,\n",
    "    output_size=1,\n",
    "    num_layers=1\n",
    ").to(device)\n",
    "\n",
    "# Train with validation and early stopping\n",
    "trained_model, train_losses, val_losses = train_with_validation(\n",
    "    rnn_model_with_val, X_train, y_train, \n",
    "    X_test, y_test,  # Using test set as validation for demonstration\n",
    "    epochs=200, \n",
    "    learning_rate=0.01,\n",
    "    patience=20\n",
    ")\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e397e67",
   "metadata": {},
   "source": [
    "## 7. Making Predictions with the RNN\n",
    "\n",
    "Now that we've trained our RNN model, let's use it to make predictions and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae96153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_plot(model, X_test, y_test, original_data, sequence_length):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Move test data to device\n",
    "    X_test = X_test.to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test).cpu().numpy().flatten()\n",
    "    \n",
    "    # Get actual test values\n",
    "    y_actual = y_test.cpu().numpy().flatten()\n",
    "    \n",
    "    # Calculate error metrics\n",
    "    mse = np.mean((y_actual - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    print(f\"Test MSE: {mse:.4f}\")\n",
    "    print(f\"Test RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot a segment of the original data\n",
    "    time_steps = np.arange(200)\n",
    "    plt.plot(time_steps, original_data[:200], 'b-', label='Original Data')\n",
    "    \n",
    "    # Plot predictions\n",
    "    offset = 800  # Start predictions after this index\n",
    "    max_points = min(150, len(y_pred))  # Limit the number of points for clarity\n",
    "    \n",
    "    pred_time_steps = np.arange(offset, offset + max_points)\n",
    "    plt.plot(pred_time_steps, y_pred[:max_points], 'r-', label='Predictions')\n",
    "    plt.plot(pred_time_steps, y_actual[:max_points], 'g-', label='Actual Values')\n",
    "    \n",
    "    plt.title('RNN Predictions vs Actual Values')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Also plot a scatter plot of actual vs predicted\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(y_actual, y_pred, alpha=0.5)\n",
    "    plt.plot([-1, 1], [-1, 1], 'r--')  # Diagonal line for reference\n",
    "    plt.title('Actual vs Predicted Values')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.grid(True)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred, y_actual\n",
    "\n",
    "# Make predictions using our trained model\n",
    "predictions, actuals = predict_and_plot(trained_model, X_test, y_test, sine_data, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93ef87",
   "metadata": {},
   "source": [
    "## 8. Implementing LSTM and GRU Models\n",
    "\n",
    "Standard RNNs often struggle with long-term dependencies due to vanishing/exploding gradient problems. Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks were designed to overcome these limitations.\n",
    "\n",
    "### LSTM Architecture\n",
    "LSTM uses three gates to regulate information flow:\n",
    "1. Forget gate: Decides what information to discard from cell state\n",
    "2. Input gate: Updates the cell state with new information\n",
    "3. Output gate: Controls what part of the cell state to output\n",
    "\n",
    "### GRU Architecture\n",
    "GRU is a simplified version of LSTM with:\n",
    "1. Reset gate: Determines how to combine new input with previous memory\n",
    "2. Update gate: Controls how much of previous memory to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53643416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True  # input shape (batch_size, seq_len, features)\n",
    "        )\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state with zeros\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        # out shape = (batch_size, seq_length, hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Use only the last time step output\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "# GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True  # input shape (batch_size, seq_len, features)\n",
    "        )\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate GRU\n",
    "        # out shape = (batch_size, seq_length, hidden_size)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        \n",
    "        # Use only the last time step output\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Initialize LSTM and GRU models\n",
    "lstm_model = LSTMModel(input_size=1, hidden_size=32, output_size=1).to(device)\n",
    "gru_model = GRUModel(input_size=1, hidden_size=32, output_size=1).to(device)\n",
    "\n",
    "# Print model architectures\n",
    "print(\"LSTM Model:\")\n",
    "print(lstm_model)\n",
    "print(\"\\nGRU Model:\")\n",
    "print(gru_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3ebcb5",
   "metadata": {},
   "source": [
    "Now let's train both the LSTM and GRU models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM model\n",
    "print(\"Training LSTM Model:\")\n",
    "lstm_model, lstm_train_losses, lstm_val_losses = train_with_validation(\n",
    "    lstm_model, X_train, y_train, \n",
    "    X_test, y_test,\n",
    "    epochs=150, \n",
    "    learning_rate=0.01,\n",
    "    patience=15\n",
    ")\n",
    "\n",
    "# Train GRU model\n",
    "print(\"\\nTraining GRU Model:\")\n",
    "gru_model, gru_train_losses, gru_val_losses = train_with_validation(\n",
    "    gru_model, X_train, y_train, \n",
    "    X_test, y_test,\n",
    "    epochs=150, \n",
    "    learning_rate=0.01,\n",
    "    patience=15\n",
    ")\n",
    "\n",
    "# Plot training losses for all models\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses, label='Simple RNN')\n",
    "plt.plot(lstm_train_losses, label='LSTM')\n",
    "plt.plot(gru_train_losses, label='GRU')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot validation losses for all models\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(val_losses, label='Simple RNN')\n",
    "plt.plot(lstm_val_losses, label='LSTM')\n",
    "plt.plot(gru_val_losses, label='GRU')\n",
    "plt.title('Validation Loss Comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Make predictions using LSTM and GRU models\n",
    "print(\"\\nLSTM Model predictions:\")\n",
    "lstm_pred, lstm_actual = predict_and_plot(lstm_model, X_test, y_test, sine_data, seq_length)\n",
    "\n",
    "print(\"\\nGRU Model predictions:\")\n",
    "gru_pred, gru_actual = predict_and_plot(gru_model, X_test, y_test, sine_data, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e409bd0f",
   "metadata": {},
   "source": [
    "## 9. Visualizing RNN Results\n",
    "\n",
    "Let's create some visualizations to better understand how our RNN models work and process sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc29f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions from all models on the same plot\n",
    "def compare_model_predictions(models, names, X_test, y_test):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Get actual values\n",
    "    y_actual = y_test.cpu().numpy().flatten()\n",
    "    \n",
    "    # Plot actual values\n",
    "    plt.plot(y_actual, 'k-', label='Actual Values', linewidth=2)\n",
    "    \n",
    "    # Plot predictions for each model\n",
    "    colors = ['r', 'g', 'b']\n",
    "    \n",
    "    for i, (model, name) in enumerate(zip(models, names)):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_test.to(device)).cpu().numpy().flatten()\n",
    "        \n",
    "        plt.plot(y_pred, colors[i]+'--', label=f'{name} Predictions', alpha=0.8)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = np.mean((y_actual - y_pred) ** 2)\n",
    "        print(f\"{name} Test MSE: {mse:.6f}\")\n",
    "    \n",
    "    plt.title('Model Predictions Comparison')\n",
    "    plt.xlabel('Test Sample')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Compare our three models\n",
    "models = [trained_model, lstm_model, gru_model]\n",
    "names = [\"Simple RNN\", \"LSTM\", \"GRU\"]\n",
    "compare_model_predictions(models, names, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the hidden state evolution for a simple RNN\n",
    "class RNNWithHidden(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNWithHidden, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, x, return_hidden=False):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # We need to save all hidden states\n",
    "        all_hidden = []\n",
    "        \n",
    "        # Process each time step manually to get all hidden states\n",
    "        if return_hidden:\n",
    "            h = h0\n",
    "            hidden_states = []\n",
    "            \n",
    "            for t in range(x.size(1)):\n",
    "                # Get input for this time step\n",
    "                x_t = x[:, t:t+1, :]\n",
    "                \n",
    "                # Run through RNN\n",
    "                out, h = self.rnn(x_t, h)\n",
    "                \n",
    "                # Save hidden state\n",
    "                hidden_states.append(h.clone())\n",
    "            \n",
    "            # Stack all hidden states\n",
    "            hidden_states = torch.cat(hidden_states, dim=0).transpose(0, 1)\n",
    "            \n",
    "            # Get final output\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            \n",
    "            return out, hidden_states\n",
    "        else:\n",
    "            # Standard forward pass\n",
    "            out, _ = self.rnn(x, h0)\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            return out\n",
    "\n",
    "# Create and train a simple model for visualization\n",
    "vis_rnn = RNNWithHidden(input_size=1, hidden_size=4, output_size=1).to(device)\n",
    "\n",
    "# Train the model\n",
    "vis_rnn_losses = train_model(vis_rnn, X_train, y_train, epochs=50)\n",
    "\n",
    "# Get a sample sequence and hidden states\n",
    "sample_idx = 5\n",
    "sample_sequence = X_test[sample_idx:sample_idx+1]\n",
    "_, hidden_states = vis_rnn(sample_sequence.to(device), return_hidden=True)\n",
    "hidden_states = hidden_states.cpu().detach().numpy()[0]  # Get the first batch\n",
    "\n",
    "# Visualize the hidden state evolution\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot each hidden unit\n",
    "for i in range(hidden_states.shape[1]):\n",
    "    plt.subplot(hidden_states.shape[1], 1, i+1)\n",
    "    plt.plot(hidden_states[:, i])\n",
    "    plt.title(f'Hidden Unit {i+1}')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Evolution of Hidden States Over Time', y=1.05, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bd46a6",
   "metadata": {},
   "source": [
    "## 10. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "1. **Basic RNN Concepts**: Understanding the architecture and how information flows through RNNs\n",
    "2. **Implementation**: Building RNNs from scratch and using PyTorch's built-in modules\n",
    "3. **Advanced Architectures**: Implementing LSTM and GRU models to handle long-term dependencies \n",
    "4. **Training and Evaluation**: Proper training procedures with validation and early stopping\n",
    "5. **Visualization**: Understanding how the hidden states evolve and model performance analysis\n",
    "\n",
    "### Next Steps for Further Learning:\n",
    "\n",
    "1. **Application to Real-World Data**: Apply RNNs to real-world sequential data like stock prices, weather data, or text data\n",
    "2. **Bidirectional RNNs**: Learn about bidirectional RNNs that process sequences in both directions\n",
    "3. **Sequence-to-Sequence Models**: Explore models for tasks like translation where both input and output are sequences\n",
    "4. **Attention Mechanisms**: Implement attention to improve performance on long sequences\n",
    "5. **Combining CNNs and RNNs**: For tasks like image captioning or video analysis\n",
    "\n",
    "### Advanced RNN Applications:\n",
    "\n",
    "- **Text Generation**: Generate human-like text by training on large text corpora\n",
    "- **Time Series Forecasting**: Predict future values in financial or scientific time series data\n",
    "- **Anomaly Detection**: Detect unusual patterns in sequential data\n",
    "- **Music Generation**: Create new music by learning patterns from existing compositions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
